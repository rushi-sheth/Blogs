{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Regression is one of the most used algorithms in Predictive analytics, but it comes with few disadvantages. In this blog, I give an overview of regularization, particularly Ridge and Lasso, for improved results and controlling model complexity.\n",
    "\n",
    "# What is regression?\n",
    "\n",
    "Regression is one of the most commonly used machine learning algorithm for supervised learning. These models are used to predict values based on the relation between dependent (target variable) and independent variables (features). \n",
    "Statistically, the linear model represents Ordinary least squares, which is given as\n",
    "\n",
    "\\begin{equation}\n",
    "y_{i}=\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\cdots+\\beta_{p} x_{i p}+\\varepsilon_{i}\n",
    "\\end{equation}\n",
    "\n",
    "It represents a kind of optimization problem, where we try to minimize our object function which is in the form of sum of squared errors. \n",
    "\n",
    "\\begin{equation}\n",
    "R S S=\\sum_{i=1}^{n}\\left(\\varepsilon_{i}\\right)^{2}=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\alpha+\\beta x_{i}\\right)\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "# Need of regularization\n",
    "\n",
    "The above given objective function performs well when the assumptions of OLS are satisfied. In a snapshot, the OLS assumptions are:\n",
    "\n",
    "1.\tNo multicollinearity\n",
    "2.\tLinear relationship between features and target\n",
    "3.\tHomoskedasticity i.e., Variance of error along X is constant\n",
    "4.\tNormal distribution of errors\n",
    "\n",
    "But with increase in data points and features, the real world data usually does not follow the OLS assumptions.\n",
    "\n",
    "Additionally, with increase in the features, the model complexity increases in an attempt to give an unbiased estimator. This lowered bias also causes over fitting. To deal with such issues, we implement regularization.\n",
    "\n",
    "# What is regularization?\n",
    "\n",
    "Regularization is a technique to reduce the bias as well as model complexity of regression models. The basic idea is to include a constraint in form of penalty term for the parameter estimate (feature coefficients) in our original objective function. \n",
    "\n",
    "# Mechanics of regularization:\n",
    "\n",
    "Earlier, our objective function was \\begin{equation}\n",
    "min\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\alpha+\\beta x_{i}\\right)\\right)^{2}\n",
    "\\end{equation} \n",
    "and our goal was to minimize it. In regularization, we still want to minimize our objective function, but the objective function now consists of a penalty term. The penalty is applied on the parameter estimates forcing it to take minimum possible values and thus decreasing the model complexity. The main difference between Ridge and Lasso regression is in selection of the penalty term.\n",
    "\n",
    "\n",
    "# Ridge Regression\n",
    "\n",
    "In ridge regression, the penalty term is of the second order, with a constraint region in shape of a circle and thus also called L2 regularization. \n",
    "\n",
    "<img src='images\\ridge.png' style=\"width:250px;height:250px\">\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "min\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\alpha+\\beta x_{i}\\right)\\right)^{2}+\n",
    "\\lambda\\|\\beta\\|_{2}^{2}\n",
    "\\end{equation}\n",
    "\n",
    "One of the issues with ridge regression is that it cannot force the coefficients to be zero, thus each coefficient is still being used in the model. Thus, ridge only helps in shrinking the coefficients and not removing it.\n",
    "\n",
    "# Lasso Regression\n",
    "### Least Absolute Shrinkage and Selection Operator\n",
    "However, in Lasso regression the penalty term is absolute sum of coefficients,resulting in a constraint region in shape of a diamond. This diamond serves as a constraint to the actual equation. It forces the coefficients to take minimum possible value. Thus “Absolute Shrinkage” in Lasso. \n",
    "\n",
    "<img src='images\\lasso.png' style=\"width:250px;height:250px\">\n",
    "<br>\n",
    "\\begin{equation}\n",
    "min\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\alpha+\\beta x_{i}\\right)\\right)^{2}+\\lambda\\|\\beta\\|_{1}\n",
    "\\end{equation}\n",
    "\n",
    "Since the contour can meet the constraint at one of the axis, the coefficient can’t take a value of zero. When this happens, it is basically removing that feature from consideration in the model. Thus the term “Selection” in LASSO.\n",
    "\n",
    "# Selection of regularization parameter\n",
    "The parameter can be identified by performing cross-validation and selecting the value of $\\beta$ that minimizes the function.\n",
    "\n",
    "# Advantages\n",
    "\n",
    "1.\tAvoids overfitting\n",
    "2.\tLasso allows shrinkage as well as variable selection\n",
    "3.\tVariable selection in Lasso provides sparse solutions\n",
    "\n",
    "# Disadvantages\n",
    "\n",
    "1.\tRidge cannot be used in variable selection\n",
    "2.\tThey increase the bias\n",
    "3.\tThey are scale variant \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
