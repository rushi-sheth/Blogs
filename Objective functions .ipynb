{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a graduate student in Industrial Engineering program, one of my first core courses was Optimization models and applications. It dealt with creating mathematical formulations of optimization problems. As I started diving deep into Machine Learning, it was easy to observe how optimization and machine learning integrate. \n",
    "\n",
    "Optimization forms the backbone of machine learning. Traditionally, the optimization model consists of some variables, constraints and one or more objective functions to be solved. In machine learning, it is this objective function which plays the important role. As we will see later, each machine learning algorithm that we run is nothing but an optimization model in form of a ___cost function___ with different objective function and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog, I try to explain the basics behind each algorithm using optimization. Additionally, there are also hyper parameters and python libraries mentioned for each algorithm.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Model Fitting and Prediction](#intro)\n",
    "- [Linear Regression](#linear)\n",
    "- [Ridge Regression](#ridge)\n",
    "- [Lasso Regression](#lasso)\n",
    "- [Logistic Regression](#logistic)\n",
    "- [Support Vector Machines](#svm)\n",
    "- [KNN](#knn)\n",
    "- [K-Means](#kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "# Understanding the model fitting and prediction process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how optimization is used in machine learning, it is important to know how prediction and classification is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is done in two parts.\n",
    "\n",
    "### Model fitting \n",
    "First, the data is fitted to a model. Fitting the model means the algorithm is actually calculating the coefficients for each independent variable and generating an equation which best describes the relation between independent and dependent variables \n",
    "\n",
    "#### But how does the algorithm find what is the best coefficient?\n",
    "This is where optimization comes into play. Each algorithm has some goal to be achieved which is described in the form of the objective. It can be minimization (for example in case of regression) or maximization (in case of SVM). \n",
    "\n",
    "The algorithms tries to achieve the best value of objective (optimal) by changing the coefficients of each independent variable. Once the optimal value is achieved, the model is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction\n",
    "\n",
    "Once the model is fitted to the data, we now have the equation describing the relation between independent and dependent variables. Next we provide our testing set as an input to this model which in turns gives the predicted value.\n",
    "\n",
    "***\n",
    "\n",
    "So our process is:\n",
    "\n",
    "1. Fit the model i.e., _model.fit()_\n",
    "2. Coefficients generated in background using _cost function_ \n",
    "3. Input the test data into this model i.e., _model.predict()_ \n",
    "4. Compare the predicted value from the model with the actual value to calculate the accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear\"></a>\n",
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistically, regression is equated by writing the dependent variable as a combination of one or more independent variables.\n",
    "\n",
    "When the combination is linear, we term it as __linear regression__.\n",
    "\n",
    "\\begin{equation}\n",
    "y_{i}=\\beta_{0} 1+\\beta_{1} x_{i 1}+\\cdots+\\beta_{p} x_{i p}+\\varepsilon_{i}=\\mathbf{x}_{i}^{\\top} \\beta+\\varepsilon_{i}\n",
    "\\end{equation}\n",
    "\n",
    "where $ y_{i}$ is the dependent variable, $x_{i}$ the independent variables and $\\beta$ are the coefficients.\n",
    "<br>\n",
    "<br>\n",
    "In machine learning, our goal is to predict the outcome of some event. So, $y_{i}$ becomes our target variable, while the features/attributes of the event becomes our independent variable. \n",
    "\n",
    "The cost function for Ordinary Least Squares (default linear regression model in scikit-learn) is the __residual sum of squares__ and the model __minimizes___ it.<br><br>\n",
    "\\begin{equation}\\min\n",
    "R S S=\\min\\sum_{i=1}^{n}\\left(\\varepsilon_{i}\\right)^{2}=\\min\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\alpha+\\beta x_{i}\\right)\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are slope and coefficients respectively.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ridge\"></a>\n",
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge and Lasso are regularization techniques that can be used to reduce overfitting. For more information, you can read my post on Regularization. \n",
    "\n",
    "The main technique is to add a penalty term to the coefficient\n",
    "so that their values are controlled. The cost function is modified accordingly to include a penalty term in the objective, and this objective as a whole is minimized.\n",
    "\n",
    "\\begin{equation}\n",
    "\\min\\left(\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{\\prime} \\hat{\\beta}\\right)^{2}+\\lambda \\sum_{j=1}^{m} \\hat{\\beta}_{j}^{2}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "__Hyper parameter__: A hyperparameter is a parameter which is an input to the model and helps in processing and generation of model.\n",
    "\n",
    "In the above equation, $\\lambda$ is the hyperparameter which helps in controlling the penalty term of the coefficients.This can be found using either cross validation or more traditional methods like AIC and BIC.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lasso\"></a>\n",
    "# Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to ridge regression, lasso regression also adds the penalty term but instead of penalizing the sum of squared coefficient, it penalizes the sum of their absolute values.\n",
    "\n",
    "\\begin{equation}\\min\\left(\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{\\prime} \\hat{\\beta}\\right)^{2}+\\lambda \\sum_{j=1}^{m}\\left|\\hat{\\beta}_{j}\\right|\\right)\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "__Hyper parameters__:Similar to ridge, $\\lambda$ is the hyperparameter controlling the regularization.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistic\"></a>\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common classification technique is logistic regression. It has the term regression in it since it's formulation is similar to regression. \n",
    "\n",
    "#### Understanding the cost function\n",
    "\n",
    "Even though the relation between target and features is defined as in regression, the main difference between logistic and linear regression is inclusion of exponential terms. The logistic regression uses the sigmoid function due to its property of giving outputs between (0,1) which are interpreted as probabilities.\n",
    "\n",
    "##### Sigmoid function\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "S(x)=\\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "##### Logistic Regression\n",
    "When we use the linear function in the sigmoid, we get logistic regression model.\n",
    "\n",
    "\\begin{equation}\n",
    "f_{w,b}(x)=\\frac{1}{1+e^{-(wx+b)}}\n",
    "\\end{equation}\n",
    "\n",
    "##### Cost Function\n",
    "Unlike linear regression, we maximize our cost function in logistic regression. The cost function for logistic regression is defined in terms of maximum likelihood. You can read more on maximum likelihood here.\n",
    "\n",
    "Since the likelihood is probability, we cannot sum it up and thus take the product of it. \n",
    "\\begin{equation}\n",
    "L_{w,b}=\\prod_{i=1..N}f_{w,b}(x_{i})^{y_{i}} (1-f_{w,b}(x_{i}))^{1-y_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "Due to the _exp_ function in the model, it is more convenient to use log likelihood and maximize it. We can also use negative log likelihood and minimize it.\n",
    "\n",
    "So the final optimization cost function is\n",
    "\n",
    "\\begin{equation}\n",
    "\\max LogL_{w,b}=\n",
    "\\max \\sum_{i=1..N}\\lbrack y_{i}\\: ln\\: f_{w,b}(x_{i}) +\n",
    "(1-y_{i})\\:ln\\:(1-f_{w,b}(x_{i}))\\rbrack\n",
    "\\end{equation}\n",
    "<br>\n",
    "__Hyper parameters__: The logistic regression does not have any hyperparameters. But a regularized logistic regression will have the parameter $\\lambda$.\n",
    "***   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"svm\"></a>\n",
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is another famous algorithm for classification as well as regression. It's basic concept is to find a best fit line which separates the data points into the respective classes. Since the focus is on loss functions, I'll only give an overview of how SVM works. The two main components for SVM are hyperplane and margins.\n",
    "\n",
    "The __hyperplane__ is the line which divides the space into distinct classes and __margin__ is the distance between this line and the closest points. Since the hyperplane divides the space, we have margin on both the sides of the plane.\n",
    "\n",
    "##### But, why the closest points? \n",
    "\n",
    "The idea behind considering the closest points from each group is that, if closest points are separated then all the other points are classified properly. This distance between the two points to the hyperplane is a perpendicular distance and since this two points define the hyperplane, they are called support vectors.\n",
    "\n",
    "<img src='images\\svm.png' ><br>\n",
    "\n",
    "Okay, so now we know what is the hyperplane, a margin and support vectors.\n",
    "Consider a weight vector __w__ and feature vector X which define the line equations. \n",
    "- The hyperplane is defined by <br><br>\n",
    "\\begin{equation}\n",
    "\\vec{w} \\cdot \\vec{x}+b=0\n",
    "\\end{equation} <br>and using optimization, we solve for __w__.\n",
    "\n",
    "\n",
    "- The __objective function__ here is to maximize the distance between both the margins. By geometry, the distance between the support vectors is given by:\n",
    "$\\frac{2}{\\|\\mathbf{w}\\|}$ \n",
    "and thus the objective becomes: <br>\n",
    "\\begin{equation}\n",
    "\\max _{\\mathbf{w}} \\frac{2}{\\|\\mathbf{w}\\|} \n",
    "\\end{equation}\n",
    "<br><BR>\n",
    "- Also, maximizing $ \\frac{2}{\\|\\mathbf{w}\\|}$ is same as minimizing ${\\|\\mathbf{w}\\|}$ . The objective function is defined in terms of \\begin{equation}\n",
    "\\min _{\\mathbf{w}}\\|\\mathbf{w}\\|^{2}\n",
    "\\end{equation} .<br><br>\n",
    "\n",
    "- The __constraint__ to this problem is that we want all the data points to be classification correctly. The data points be classified into classes -1 and 1, depending on which side of the margin they are.<br> So, our constraints are defined as:\n",
    "<br><BR>\n",
    "\\begin{equation}\n",
    "\\begin{array}{l}{x_{i} \\cdot w+b \\geq+1 \\text { when } y_{i}=+1} \\\\ {x_{i} \\cdot w+b \\leq-1 \\text { when } y_{i}=-1}\\end{array} \\\\ \\text {or} \\\\ y_{i}\\left(x_{i} . w + b\\right) \\geq 1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Soft Margin vs Hard Margin\n",
    "\n",
    "The optimization objective we discussed above assumes that we have linearly separable data. But most of the time our data is not linearly separable. To solve such problem, we use the concept of soft margin.\n",
    "\n",
    "A soft margin is when we allow the model to make few mistakes in an attempt to find a best hyperplane. But we also want to make sure that we control the number of mistakes, which is done by including a penalty term 'C'.\n",
    "\\begin{equation}\n",
    "\\min _{\\mathbf{w} \\in \\mathbb{R}^{d}, \\xi_{i} \\in \\mathbb{R}^{+}}\\|\\mathbf{w}\\|^{2}+C \\sum_{i}^{N} \\xi_{i} \\\\ \\text { subject to } y_{i}\\left(\\mathbf{w}^{\\top} \\mathbf{x}_{i}+b\\right) \\geq 1-\\xi_{i} \\text { for } i=1 \\ldots N \n",
    "\\end{equation}\n",
    "\n",
    "The penalty is the link between the size of the margin and our tolerance on number of mistakes. If the tolerance is low, i.e., we care about the number of mistakes then we keep the penalty high. A high penalty will make less mistakes since the optimization problem goes towards the original hard margin problem.\n",
    "\n",
    "__Understanding the penalty function__\n",
    "\n",
    "\n",
    "1. Let's say our data point to be classified is +1 and our original positive margin is (wX+b)=1. \n",
    "2. Our ideal case is when the data is correctly classified. In that case, our margins are correct and the penalty should be zero. \n",
    "3. If this point is misclassified, then it means that it lies on the other side of hyperplane and the positive margin.\n",
    "4. Assume it is at distance 'd' from the original positive margin.\n",
    "5. For this misclassified point to be classified correctly, we will have to shift the positive margin towards the point until it is correctly classified, i.e., distance d.\n",
    "6. By moving the margin towards hyperplane by distance d changes our margin equation to y(wX+b) = 1-d. Thus, d=1-y(wX+b), which is the distance that we are moving for correcting the misclassified point. \n",
    "7. So our distance ranges from 0 if correct to 1-y(wX+b) if incorrect. This is represented as max(0,1-y(wX+b)). This function is called hinge loss function and is represented as below.\n",
    "\n",
    "\\begin{equation}\n",
    "\\xi_{i}=\\max \\left(0,1- y_{i}\\left(x_{i} . w + b\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "We then use this hinge loss function with a regularization variable \"C\" to controls the trade off of margin size and misclassification. As discussed, if C is higher, then this problem will be almost like a hard margin problem and have a high misclassification, while if it is very small, it will ignore the data since the 'd' can now be set to any value. The term \"C\" is our __hyper parameter__ which can be identified using cross validation.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is another classification and regression method. As the name suggests, it uses the nearest neighbors to classify or predict the data points.\n",
    "\n",
    "An important feature of KNN is that it is a non-parametric, instance based learning algorithm. In instance based learning the algorithm does not generate a model defining the relation between the target and feature, but instead stores the training data in memory and uses it every time a new point is to be predicted or classified. \n",
    "\n",
    "Okay, that makes sense. How does it work then? <br><br>\n",
    "KNN first looks for data points in the training data which are similar to the test data point in consideration. It then takes the average(in case of regression) or the most common label (in case of classification) and assigns it to the test data. \n",
    "\n",
    "These similar data points are called the nearest neighbors and are identified using various distance metrics. The number of neighbors also plays an important role since the mode or average is to be taken. This number is called _k_ and it is the hyperparameter in this algorithm. If k=1, then we are comparing the point with the nearest point only and this results in overfitting. The best value of k can be identified through testing or tuning methods like cross validation.\n",
    "\n",
    "Pseudo code:\n",
    "1. Keep the training data in the memory\n",
    "2. Specify the value of K and the distance to be used\n",
    "3. Give test data as input\n",
    "4. Calculate the nearest _k_ points using the specified distance metrics\n",
    "5. Label the test data with the most common label within the range of distance metrics.\n",
    "\n",
    "##### Distance Metrics \n",
    "\n",
    "There are few options that we can use to calculate the distance between test and training point. These are:\n",
    "\n",
    "1. **Minkowski Distance** -  A generalized formula of distance metrics.\n",
    "\\begin{equation}\n",
    "d(\\mathbf{a},\\mathbf{b}) =\n",
    "\\left(\\sum_{i=1}^{n}\\left|a_{i}-b_{i}\\right|^{p}\\right)^{1 / p}\n",
    "\\end{equation}\n",
    "2. **Manhanttan Distance**-A special case of Minkowski distance, p=1.Preferred if the data is of different type (age,gender,height etc.)\n",
    "\\begin{equation}\n",
    "d=\\sum_{i=1}^{n}\\left|a_{i}-b_{i}\\right|\n",
    "\\end{equation}\n",
    "3. **Euclidean distance**- A special case of Minkowski distance, p=2. <br>Preferred if data is of similar type (width,height etc)\n",
    "\\begin{equation}\n",
    " d(\\mathbf{a}, \\mathbf{b})=\\sqrt{\\sum_{i=1}^{n}\\left(a_{i}-_{i}\\right)^{2}} \n",
    "\\end{equation}\n",
    "4. **Hamming distance**: The above three distances are used for continuous variables. In case of categorical variables, the distance is zero if both two values are equal and 1 id otherwise.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kmeans\"></a>\n",
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previously discussed algorithms, the K-Means algorithm is an unsupervised machine learning algorithm used for clustering tasks. I will be skipping the introduction of the algorithm and move to the objective function. \n",
    "\n",
    "- **Objective**: The objective function of the K-Means is to reduce the sum of squared error within the clusters.\n",
    "- Once the initial centroids are generated, the data points are assigned to each centroid thus forming a cluster. Similar to K-NN, the distance is measured from the data point to centroid to identify which cluster it belongs to. \n",
    "- Once this assignment is done, we update the centroid (usually we define the centroid as mean of the cluster) and recalculate the distances. \n",
    "- This is repeated till the SSE of each cluster is minimized or when the centroid stops updating.\n",
    "\n",
    "**Assignment**: \\begin{equation}\n",
    "\\underset{c_{i} \\in C}{\\operatorname{argmin}} \\operatorname{dist}\\left(c_{i}, x\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "**Centroid Update**: \\begin{equation}\n",
    "c_{k}=\\frac{1}{\\left|S_{k}\\right|} \\sum_{x_{i} \\in S_{k}} x_{i}\n",
    "\\end{equation}, where S is the set of data points assigned to cluster $c_{k}$\n",
    "\n",
    "**Objective**: \\begin{equation}\n",
    "=\\sum_{k=1}^{k} \\sum_{x_{i} \\in C_{k}}\\left(x_{i}-c_{k}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "- k is the number of clusters \n",
    "- $x_{i}$ is the data point within the cluster $c_{k}$\n",
    "- $c_{k}$ is the centroid location or mean value of centroid\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
